if __name__ == '__main__':
쓰는 이유

스크립트가 파이썬 인터프리터 명령어로 패싱되어 실행되면(python myscript.py같이) 다른 언어들과는 다르게, 파이썬은 자동으로 실행되는 메인함수가 없습니다. 파이썬은 메인 함수가 없는 대신 들여쓰기 하지 않은 모든 코드(level 0코드)를 실행합니다 다만, 함수나 클래스는 정의되었지만, 실행되지는 않습니다

질문하신 경우, 최 상위 코드는 if 블록이고, __name__은 현재 모듈의 이름을 담고있는 내장 변수입니다. python myscript.py 같이 이 모듈이 직접 실행되는 경우에만,__name__ 은 "__main__"으로 설정됩니다.

따라서 질문자의 코드가 다른 모듈에 의해 import된 경우 함수와 객체의 정의는 import되지만 __name__이 "__main__"이 아니기 때문에 if문은 실행되지 않습니다.

이처럼,

if __name__ == "__main__":
을 써서, 스크립트가 직접 실행되는지 혹은 import되어 실행되는지 테스트 할 수 있습니다.

예를들어 다음 코드의 경우

file A.py
def func():
    print("function A.py")

print("top-level A.py")

if __name__ == "__main__":
    print("A.py 직접 실행")
else:
    print("A.py가 임포트되어 사용됨")
file B.py
import A

print(""top-level in B.py"")
one.func()

if __name__ == "__main__":
    print("B.py가 직접 실행")
else:
    print("B.py가 임포트되어 사용됨")

>> python A.py
top-level in A.py
A.py가 직접 실행


>>python B.py
top-level in A.py
A.py가 임포트되어 사용됨
top-level in B.py
function A.py
B.py가 직접 실행


//////////////////////
requests.get() 은
Python에서 HTTP 요청을 보내는 모듈인 requests를 이용


import
requests URL = 'http://www.tistory.com'
response = requests.get(URL)
response.status_code
response.text

1. GET 요청할 때 parameter 전달법
params = {'param1': 'value1', 'param2': 'value'}
res = requests.get(URL, params=params)

2. POST 요청할 때 data 전달법
위의 내용과 같다, params 대신 data라는 이름으로 주면 된다.

data = {'param1': 'value1', 'param2': 'value'}
res = requests.post(URL, data=data)

조금 더 복잡한 구조로 POST 요청을 해야 할 때가 있다. 이럴 때는 위의 방법처럼 순진하게 주면 안된다. 우리가 인지하고 있는 그 딕셔너리의 구조를 유지하면서 문자열로 바꿔서 전달해줘야 하는데(?), python에서 이 노동을 해주는 친구가 json 모듈이다.

import requests, json
data = {'outer': {'inner': 'value'}}
res = requests.post(URL, data=json.dumps(data))

3. 헤더 추가, 쿠키 추가
별도의 헤더 옵션을 추가하고자 할 때는 headers 옵션을, 쿠키를 심어서 요청을 보내고 싶으면 cookies 옵션을 사용하면 된다

headers = {'Content-Type': 'application/json; charset=utf-8'}
cookies = {'session_id': 'sorryidontcare'}
res = requests.get(URL, headers=headers, cookies=cookies)
4. 응답(Response) 객체
요청(request)을 보내면 응답(response)을 받는다. 당연히 이 응답은 python 객체로 받는다. 그리고 이 응답 객체는 많은 정보와 기능을 가지고 있다. ipython이나 jupyter notebook에서 <탭> 기능을 이용해서 직접 체험해보면 금방 파악이 가능하지만 여기에 몇 가지만 기록하겠다.

python-requests-response-object
ipython 환경에서 res.<탭>을 통해 어떤 요소 및 함수가 있는지 살펴볼 수 있다.

res.request # 내가 보낸 request 객체에 접근 가능
res.status_code # 응답 코드
res.raise_for_status() # 200 OK 코드가 아닌 경우 에러 발동
res.json() # json response일 경우 딕셔너리 타입으로 바로 변환

/////
requests모듈 공식 홈페이지에 나와있는 사용법이다. 사용하기 매우 단순해 보인다.

>>> r = requests.get('https://api.github.com/user', auth=('user', 'pass'))
>>> r.status_code
200
>>> r.headers['content-type']
'application/json; charset=utf8'
>>> r.encoding
'utf-8'
>>> r.text
u'{"type":"User"...'
>>> r.json()
{u'private_gists': 419, u'total_private_repos': 77, ...}

아래와 같이 함수를 만들어두고 사용하도록 하겠다.

def get_html(url):
   _html = ""
   resp = requests.get(url)
   if resp.status_code == 200:
      _html = resp.text
   return _html


. Beautiful soup

많이 쓰이는 파이썬용 파서로, html, xml을 파싱할때 주로 많이 사용한다. lxml을 사용하면 성능 향상이 있다고 공식 홈페이지에 나와있다.

from bs4 import BeautifulSoup
URL = "http://comic.naver.com/webtoon/list.nhn?titleId=20853&weekday=tue&page=1"
html = get_html(URL)
soup = BeautifulSoup(html, 'html.parser')

미리 만들어어둔 get_html 함수를 이용해 html을 얻은 후 beautifulSoup을 이용해서 파싱객체를 생성한다. 위의 예에서는 soup이라는 객체가 파싱결과를 담고 있다.

soup객체는 내가 필요한 부분을 쉽게 찾을수 있도록 find, find_all함수를 제공한다. 위의 결과에서 a 태그를 모두 찾기 위해서 soup.find_all(“a”)라는 함수만 실행하면 된다.

>>> l = soup.find_all("a")
>>> print(len(l))
127

확인결과 table-> tbody-> tr -> td에 위치 하고 있는 것으로 보인다. html문서에서 한번에 결과를 얻어오려면 복잡해지기 때문에 table태그의 class가 viewList인 html조각 객체를 얻어온다.

webtoon_area = soup.find("table",{"class": "viewList"})
이제 class가 viewList인 table 태그의 bs객체를 얻었다. 이 객체에 포함되어 있는 td 태그에 class가 title 인 모든 객체를 얻기 위해서는 find_all함수를 사용하면 된다.

webtoon_area = soup.find("table",
      {"class": "viewList"}
        ).find_all("td", {"class":"title"})
find_all은 결과를 리스트로 반환하므로, for ~in 문으로 순회하면서 원하는 처리를 하면 된다.

def parse_html(html):
	"""
	입력받은 마음의 소리 웹툰 페이지 html에서 마음의소리의 회차, 제목 url을 추출하여
	tuple로 만들고, 리스트에 갯수대로 저장하여 반환한다
	:param html: string
	:return: 마음의 소리 정보가 담긴 리스트
	"""
	webtoon_list = list()
	soup = BeautifulSoup(html, 'html.parser')
	webtoon_area = soup.find("table",
			{"class": "viewList"}
	        ).find_all("td", {"class":"title"})
	for webtoon_index in webtoon_area:
		info_soup = webtoon_index.find("a")
		_url = info_soup["href"]
		_text = info_soup.text.split(".")
		_title  = ""
		_num = _text[0]
		if len(_text) > 1:
			_title = _text[1]

		webtoon_list.append((_num, _title, _url, ))
	return webtoon_list


# CSS Selector를 통해 html요소들을 찾아낸다.
my_titles = soup.select(
    'h3 > a'
    )

////
LookupError:
**********************************************************************
  Resource punkt not found.
  Please use the NLTK Downloader to obtain the resource:

  >>> import nltk
  >>> nltk.download('punkt')
에러 조치
# nltk.download()
통해서 조치 다운

////
import time

time.sleep(10)  # 10초 멈추기

////
TypeError: expected string or bytes-like object
잘못된 형식
str () 해줌